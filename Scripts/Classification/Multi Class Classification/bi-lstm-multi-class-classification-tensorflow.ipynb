{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiclass Classification using Bi-GRU and Embedding Matrix with Glove\n",
    "\n",
    "## Dataset\n",
    "- https://www.kaggle.com/stefanlarson/outofscope-intent-classification-dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "pd.set_option('display.max_rows', 700)\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_json('/kaggle/input/outofscope-intent-classification-dataset/is_train.json')\n",
    "val = pd.read_json('/kaggle/input/outofscope-intent-classification-dataset/is_val.json')\n",
    "test = pd.read_json('/kaggle/input/outofscope-intent-classification-dataset/is_test.json')\n",
    "oos_train = pd.read_json('/kaggle/input/outofscope-intent-classification-dataset/oos_train.json')\n",
    "oos_val = pd.read_json('/kaggle/input/outofscope-intent-classification-dataset/oos_val.json')\n",
    "oos_test = pd.read_json('/kaggle/input/outofscope-intent-classification-dataset/oos_test.json')\n",
    "files = [(train,'train'),(val,'val'),(test,'test'),(oos_train,'oos_train'),(oos_val,'oos_val'),(oos_test,'oos_test')]\n",
    "for file,name in files:\n",
    "    file.columns = ['text','intent']\n",
    "    print(f'{name} shape:{file.shape}, {name} has {train.isna().sum().sum()} null values')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from string import digits\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from tensorflow.keras.layers import Dense, Embedding, LSTM, SpatialDropout1D, Bidirectional, GRU\n",
    "from sklearn.preprocessing import MultiLabelBinarizer, LabelEncoder, OneHotEncoder\n",
    "from tensorflow.keras.models import Sequential\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = train.append(test)\n",
    "df.shape, train.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.intent.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "li = ['play_music', 'smart_home', 'current_location', 'tell_joke', 'next_song',  'flip_coin', 'greeting', 'what_song', 'calendar', 'time']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sample(frac=1).reset_index(drop=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ = pd.DataFrame()\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    if row['intent'] in li:\n",
    "        df_ = df_.append(row.to_dict(), ignore_index = True)\n",
    "df_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_.intent.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(df):\n",
    "    # convert source and target text to Lowercase\n",
    "    df['text'] = df['text'].apply(lambda x: x.lower())\n",
    "    \n",
    "    # creating a space between a word and the punctuation following it\n",
    "    df['text'] = df['text'].apply(lambda x: re.sub(r\"([?.!,¿])\", r\" \\1 \", x))\n",
    "    df['text'] = df['text'].apply(lambda x: re.sub(r'[\" \"]+', \" \", x))\n",
    "    \n",
    "    # replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")\n",
    "    df['text'] = df['text'].apply(lambda x: re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", x))\n",
    "    \n",
    "    # Remove digits from source and target sentences\n",
    "    num_digits = str.maketrans('', '', digits)\n",
    "    df['text'] = df['text'].apply(lambda x: x.translate(num_digits))\n",
    "    \n",
    "    # Remove extra spaces\n",
    "    df['text'] = df['text'].apply(lambda x: x.strip())\n",
    "\n",
    "    df['text'] = df['text'].apply(lambda x: re.sub(\" +\", \" \", x))\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = preprocess(df_)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating instance of labelencoder\n",
    "labelencoder = LabelEncoder()\n",
    "# Assigning numerical values and storing in another column\n",
    "df['le_intent'] = labelencoder.fit_transform(df['intent'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(labelencoder.classes_)\n",
    "np.save('classes.npy', labelencoder.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = to_categorical(df['le_intent'], num_classes=len(li))\n",
    "print(labels[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_most_common_words = 8000\n",
    "# max_len = 130\n",
    "# tokenizer = Tokenizer(num_words=n_most_common_words, filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~', lower=True)\n",
    "# tokenizer.fit_on_texts(df['text'].values)\n",
    "# sequences = tokenizer.texts_to_sequences(df['text'].values)\n",
    "# word_index = tokenizer.word_index\n",
    "# print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "# X = pad_sequences(sequences, maxlen=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_most_common_words = 8000\n",
    "max_len = 40\n",
    "def tokenization(data, maxlength = 100):\n",
    "    token = Tokenizer(num_words=n_most_common_words, lower=True, oov_token='oov')\n",
    "    token.fit_on_texts(data)\n",
    "    \n",
    "    data_seq = token.texts_to_sequences(data)\n",
    "    data_pad = pad_sequences(data_seq, maxlen=maxlength, padding='post')\n",
    "    \n",
    "    return token, data_pad\n",
    "\n",
    "token, X = tokenization(df['text'].values, maxlength=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reverse_word_index = {v: k for k, v in token.word_index.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_glove = '/kaggle/input/glove6b200d/glove.6B.200d.txt'\n",
    "\n",
    "# creating glove vectors\n",
    "def get_glove_vector():\n",
    "    glove_vectors = {}\n",
    "\n",
    "    with open(path_glove, \"r\", encoding=\"UTF-8\") as glove:\n",
    "        for line in glove:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            vectors = np.asarray(values[1:])\n",
    "            glove_vectors[word] = vectors\n",
    "    return glove_vectors\n",
    "\n",
    "glove_vectors = get_glove_vector()\n",
    "total_words = len(glove_vectors.keys()) \n",
    "total_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_dim = 200\n",
    "\n",
    "\n",
    "# create word vector matrix with glove vectors\n",
    "def create_word_vector_matrix(token, glove_vectors, vocab_size, emb_dim):\n",
    "    word_vector_matrix = np.zeros((vocab_size+1, emb_dim))\n",
    "    \n",
    "    count = 0\n",
    "    for word, index in token.word_index.items():\n",
    "        vector = glove_vectors.get(word)\n",
    "        if vector is not None:\n",
    "            word_vector_matrix[index] = vector\n",
    "        else:\n",
    "            count += 1\n",
    "    print(f\"Vector not found for {count} words\")\n",
    "    return word_vector_matrix\n",
    "\n",
    "vocab_size = len(token.word_index) \n",
    "emb_matrix = create_word_vector_matrix(token, glove_vectors, vocab_size, emb_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(emb_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Saving emb_matrix\")\n",
    "np.save('emb_matrix.npy', emb_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X , labels, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_dim = 200\n",
    "batch_size = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_size+1, \n",
    "                    output_dim=emb_dim, \n",
    "                    input_length=X.shape[1],\n",
    "                    weights=[emb_matrix],\n",
    "                    trainable=False))\n",
    "# model.add(Dropout(0.2))\n",
    "# model.add(Input(shape=(50,)))\n",
    "model.add(Bidirectional(GRU(128)))\n",
    "# model.add(Dense(units=256, activation='relu'))\n",
    "# model.add(Dropout(0.2))\n",
    "model.add(Dense(units=len(li), activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print((X_train.shape, y_train.shape, X_test.shape, y_test.shape))\n",
    "\n",
    "# model = Sequential()\n",
    "# model.add(Embedding(n_most_common_words, emb_dim, input_length=X.shape[1]))\n",
    "# model.add(SpatialDropout1D(0.7))\n",
    "# model.add(LSTM(64, dropout=0.7, recurrent_dropout=0.7))\n",
    "# model.add(Dense(150, activation='softmax'))\n",
    "# model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])\n",
    "# print(model.summary())\n",
    "# history = model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size,validation_split=0.2,callbacks=[EarlyStopping(monitor='val_loss',patience=7, min_delta=0.0001)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q tensorgram\n",
    "from tensorgram import TensorGram\n",
    "token_data = \"479470573\"\n",
    "tg=TensorGram(\"LSTM\",token_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_filepath = '/tmp/checkpoint'\n",
    "model_checkpoint_callback = ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    save_weights_only=True,\n",
    "    monitor='val_accuracy',\n",
    "    mode='max',\n",
    "    save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 40\n",
    "history = model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size,validation_split=0.2,callbacks=[EarlyStopping(monitor='val_loss',patience=7, min_delta=0.0001),\n",
    "                                                                                                            model_checkpoint_callback, \n",
    "                                                                                                           tg])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accr = model.evaluate(X_test,y_test)\n",
    "print('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}'.format(accr[0],accr[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_graphs(history, string):\n",
    "  plt.plot(history.history[string])\n",
    "  plt.plot(history.history['val_'+string])\n",
    "  plt.xlabel(\"Epochs\")\n",
    "  plt.ylabel(string)\n",
    "  plt.legend([string, 'val_'+string])\n",
    "  plt.show()\n",
    "  \n",
    "plot_graphs(history, \"accuracy\")\n",
    "plot_graphs(history, \"loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"model/BI-GRU.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir('model/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans = model.predict(X_test[0])\n",
    "print(ans.shape)\n",
    "print(labels[np.argmax(ans)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# saving\n",
    "with open('tokenizer.pickle', 'wb') as handle:\n",
    "    pickle.dump(token, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "classes = np.load('classes.npy', allow_pickle=True)\n",
    "with open('tokenizer.pickle', 'rb') as handle:\n",
    "    tokenizer = pickle.load(handle)\n",
    "\n",
    "model = load_model('model/BI-GRU.h5')\n",
    "\n",
    "new_complaint = ['how is everything going for you']\n",
    "# seq = tokenizer.texts_to_sequences(new_complaint)\n",
    "\n",
    "\n",
    "n_most_common_words = 8000\n",
    "max_len = 40\n",
    "\n",
    "\n",
    "def tokenization(data, maxlength=100):\n",
    "    token = Tokenizer(num_words=n_most_common_words, lower=True, oov_token='oov')\n",
    "    token.fit_on_texts(data)\n",
    "\n",
    "    data_seq = token.texts_to_sequences(data)\n",
    "    data_pad = pad_sequences(data_seq, maxlen=maxlength, padding='post')\n",
    "\n",
    "    return token, data_pad\n",
    "\n",
    "\n",
    "token, seq = tokenization(new_complaint, maxlength=max_len)\n",
    "padded = pad_sequences(seq, maxlen=max_len)\n",
    "pred = model.predict(padded)\n",
    "res = np.argmax(model.predict(padded), axis=-1)\n",
    "print(classes[res[0]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check save files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
